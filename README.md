# Pretraining-a-transformer-model-from-scratch-on-Wikidata
Unsupervised pretraining a Transformer GPT-2 style causal language model from scratch using Wikidata
